import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib

# ==========================================
# AYARLAR: HASSASÄ°YET PARAMETRELERÄ°
# ==========================================
DEFAULT_SIGMA = 4    # DiÄŸer sensÃ¶rler iÃ§in Standart Sapma Ã‡arpanÄ±
TEMP_SIGMA = 15      # SÄ±caklÄ±k sensÃ¶rÃ¼ iÃ§in GeniÅŸletilmiÅŸ Ã‡arpan

# Modelin anomali hassasiyeti (%5)
CONTAMINATION_RATE = 0.05 

print("[BÄ°LGÄ°] Veri seti yÃ¼kleniyor ve Ã¶n iÅŸleme yapÄ±lÄ±yor...")

try:
    df_raw = pd.read_csv('sensor_data.csv', header=None, na_values=['\\N'])
    df_real = df_raw[[2, 3, 4, 5, 6, 7, 8]].copy()
    df_real.columns = ['voltage', 'current', 'sound', 'gas', 'temp', 'hum', 'heading']
    df_real = df_real.apply(pd.to_numeric, errors='coerce')
    df_real.dropna(inplace=True)
    
    print(f"âœ… GerÃ§ek Veri Seti HazÄ±r: {len(df_real)} Ã¶rnek.")

except FileNotFoundError:
    print("âŒ HATA: 'sensor_data.csv' dosyasÄ± bulunamadÄ±.")
    exit()

# ==========================================
# 2. ADIM: SENTETÄ°K ANOMALÄ° ÃœRETÄ°MÄ°
# ==========================================
print("[BÄ°LGÄ°] Model eÄŸitimi iÃ§in sentetik anomali verileri Ã¼retiliyor...")

anomali_listesi = []

for col in df_real.columns:
    mu = df_real[col].mean()
    sigma = df_real[col].std()
    
    if sigma == 0: sigma = mu * 0.1 if mu != 0 else 1
    
    # SensÃ¶r tipine gÃ¶re tolerans katsayÄ±sÄ± belirleme
    if col == 'temp':
        katsayi = TEMP_SIGMA
    else:
        katsayi = DEFAULT_SIGMA
    
    limit_ust = mu + (katsayi * sigma)
    limit_alt = mu - (katsayi * sigma)
    
    # Anomali Ã–rneklem SayÄ±sÄ± (DÃ¼ÅŸÃ¼k tutularak modelin bunlarÄ± aykÄ±rÄ± gÃ¶rmesi saÄŸlanÄ±r)
    SAMPLE_SIZE = 5 
    
    # 1. Ãœst Limit SapmasÄ±
    high_anomali = df_real.sample(SAMPLE_SIZE).copy()
    high_anomali[col] = np.random.uniform(limit_ust, limit_ust * 1.5, SAMPLE_SIZE)
    anomali_listesi.append(high_anomali)
    
    # 2. Alt Limit SapmasÄ±
    low_anomali = df_real.sample(SAMPLE_SIZE).copy()
    val_min = limit_alt * 1.5 if col == 'temp' else max(0, limit_alt * 0.5)
    low_anomali[col] = np.random.uniform(val_min, limit_alt, SAMPLE_SIZE)
    anomali_listesi.append(low_anomali)

# Veri setlerini birleÅŸtirme
df_anomali = pd.concat(anomali_listesi, ignore_index=True)
df_final = pd.concat([df_real, df_anomali], ignore_index=True)
df_final = df_final.sample(frac=1).reset_index(drop=True)

print(f"[BÄ°LGÄ°] EÄŸitim Seti: {len(df_real)} Normal + {len(df_anomali)} Anomali Verisi")

# ==========================================
# 3. ADIM: MODEL EÄÄ°TÄ°MÄ°
# ==========================================
print("[BÄ°LGÄ°] Isolation Forest algoritmasÄ± eÄŸitiliyor...")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_final)

# Modelin oluÅŸturulmasÄ± ve eÄŸitilmesi
model = IsolationForest(n_estimators=100, contamination=CONTAMINATION_RATE, random_state=42)
model.fit(X_scaled)

# ==========================================
# 4. ADIM: SONUÃ‡LARIN KAYDEDÄ°LMESÄ°
# ==========================================
df_final['tahmin'] = model.predict(X_scaled)
df_final['durum'] = df_final['tahmin'].apply(lambda x: 'ANOMALI' if x == -1 else 'NORMAL')
df_final.to_csv('tugba_icin_veri.csv', index=False)

joblib.dump(model, 'model.pkl')
joblib.dump(scaler, 'scaler.pkl')

print("-" * 50)
print("âœ… MODEL EÄÄ°TÄ°MÄ° BAÅARIYLA TAMAMLANDI.")
print("ğŸ“ OluÅŸturulan Dosyalar:")
print("   - model.pkl (EÄŸitilmiÅŸ Model)")
print("   - scaler.pkl (Ã–lÃ§ekleyici)")
print("   - tugba_icin_veri.csv (Analiz Verisi)")
print("-" * 50)